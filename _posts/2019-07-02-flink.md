---
layout: post
title: "From Spark to Flink"
date: 2019-07-02
---

Since the beginning of Flare development, the focus was on accelerating Spark SQL. But is Flare tailored for Spark? Not at all. Indeed, Flare code base is made of generic collections (e.g. Buffers, HashMaps, etc.) and Operators (e.g. Aggregate, Joins, Windows, etc.). The Spark-to-Flare compiler extracts the query plan generating by Spark and creates the corresponding Flare operator!

A Spark query plan looks as follow:

```
Sort [l_returnflag#122 ASC NULLS FIRST, l_linestatus#123 ASC NULLS FIRST], true
+- Aggregate [l_returnflag#122, l_linestatus#123],
        [l_returnflag#122, l_linestatus#123,
        sum(l_quantity#118) AS sum_qty#374,
        sum(l_extendedprice#119) AS sum_base_price#375,
        sum((l_extendedprice#119 * (1.0 - l_discount#120))) AS sum_disc_price#376,
        sum(((l_extendedprice#119 * (1.0 - l_discount#120)) * (1.0 + l_tax#121))) AS sum_charge#377,
        avg(l_quantity#118) AS avg_qty#378,
        avg(l_extendedprice#119) AS avg_price#379,
        avg(l_discount#120) AS avg_disc#380,
        count(1) AS count_order#381L]
   +- Project [l_quantity#118, l_extendedprice#119, l_discount#120, l_tax#121, l_returnflag#122, l_linestatus#123]
      +- Filter (l_shipdate#124 <= 10471)
         +- LogicalRDD [l_orderkey#114, l_partkey#115, l_suppkey#116, l_linenumber#117, l_quantity#118, l_extendedprice#119, l_discount#120, l_tax#121, l_returnflag#122, l_linestatus#123, l_shipdate#124, l_commitdate#125, l_receiptdate#126, l_shipinstruct#127, l_shipmode#128, l_comment#129], false
```

A Spark-to-Flare compiler needs to be able to create the different Flare schemas and transform the Spark expressions to Flare expressions!

Therefore, it is possible to create an X-to-Flare compiler for any system that generates relational query plans with enough type information. We now present our latest proof of concept: Flink-to-Flare compiler!

Similarly to Spark, the Flink query plan contains field/type information. Even if the operators are different, for example, Flink merges Project and Filters into a single node (see below), the query plan is very similar! Indeed, this query plan is for the first query of TPC-H, and Flare accelerates Spark and Flink the same way on that query!

Flink query plan:
```
== Optimized Logical Plan ==
DataSetSort(orderBy=[l_returnflag ASC, l_linestatus ASC])
  DataSetAggregate(
    groupBy=[l_returnflag, l_linestatus],
    select=[l_returnflag, l_linestatus,
              SUM(l_quantity) AS sum_qty,
              SUM(l_extendedprice) AS sum_base_price,
              SUM($f4) AS sum_disc_price,
              SUM($f5) AS sum_charge,
              AVG(l_quantity) AS avg_qty,
              AVG(l_extendedprice) AS avg_price,
              AVG(l_discount) AS avg_disc,
              COUNT(*) AS count_order])
    DataSetCalc(
        select=[l_returnflag, l_linestatus, l_quantity, l_extendedprice, *(l_extendedprice, -(1, l_discount)) AS $f4, *(*(l_extendedprice, -(1, l_discount)), +(1, l_tax)) AS $f5, l_discount],
        where=[<=(l_shipdate, 1998-09-02)])
      BatchTableSourceScan(
          table=[[lineitem]],
          fields=[l_returnflag, l_linestatus, l_quantity, l_extendedprice, l_discount, l_tax, l_shipdate],
          source=[CsvTableSource(read fields: l_returnflag, l_linestatus, l_quantity, l_extendedprice, l_discount, l_tax, l_shipdate)])
```

## Evaluation

We evaluated how Flare accelerates Flink on the full TPCH benchmark. The data is streamed from a CSV file. The performances of Flare are very similar whichever front-end system is used. However, Flink does not generate semi/anti-joins but used equivalent aggregates on Boolean that are slightly slower (see Q18, Q22).

 <div>
<img src="{{ site.baseurl }}/img/tpch-flink-comparison.png" width="100%" />
</div>
